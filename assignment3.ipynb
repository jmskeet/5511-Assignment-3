{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2b4ed2",
   "metadata": {},
   "source": [
    "## Assignment 3: CNN Cancer Detection Kaggle Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a72b7",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    " \n",
    "This project is a binary image classification.  The goal of this project is to develop a convolutional nueral network model that can accurately identify metastatic cancer patches in an image.  The images are provided by the Kaggle Histopathologic Cancer Detection Competition and located at https://www.kaggle.com/competitions/histopathologic-cancer-detection/data.  \n",
    "\n",
    "### Data\n",
    "\n",
    "See Images 1 and 2 for examples of positive and negative instances, respectively.  A review of the images, image ids and  and ground truth labels showed no anomolous cases.\n",
    "\n",
    "The data consists of training and test images with image ids provided as the filename.  There are 220,025 training images and 57,458 test images. The train_labels.csv contains 220,025 rows that house the image id and assoiciated ground truth lables for the training images.  The sample_submission.csv contains the image ids of the test images and sampleground truth labels.  The labeles are to be replaced with test results and submitted for assessment of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ce405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Required Resources\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "import cv2\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c697e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "train_image_df = pd.read_csv(\"/train/train_labels.csv\")\n",
    "test_image_df = pd.read_csv(\"/test/sample_submission.csv\")\n",
    "\n",
    "print(train_image_df.head(), '\\n')\n",
    "print(train_image_df.info(), '\\n')\n",
    "print(test_image_df.head(), '\\n')\n",
    "print(test_image_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb81aaf",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "See Images 1 and 2 for examples of positive and negative instances, respectively.  A review of the images, image ids and  and ground truth labels showed no anomolous cases.\n",
    "\n",
    "The data consists of training and test images with image ids provided as the filename.  There are 220,025 training images and 57,458 test images. The train_labels.csv contains 220,025 rows that house the image id and assoiciated ground truth lables for the training images.  The sample_submission.csv contains the image ids of the test images and sampleground truth labels.  The labeles are to be replaced with test results and submitted for assessment of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Sample Images\n",
    "\n",
    "fig, ax = plt.subplots(15, 4, figsize = (15, 15))\n",
    "ax[0,1].imshow(cv2.imread('/train/' + train_image_df[0]['id] + 'tif'))\n",
    "ax[1,1].imshow(cv2.imread(/train/ + train_image_df[1]['id] + 'tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aea4d6",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "As shown in Charts 1 and 2 59.5% of the training examples are negative while 40.5% positive.  This imbalance could result in biased trainig so the training set is rebalanced to a 50-50 split. Chart 3 shows the new distribution to be used as the training set.  As noted above, a review of the images, image ids and labels revealed no anomolies.  No addtional data cleansing or adjustments are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charts, data shape\n",
    "\n",
    "\n",
    "\n",
    "# Rebalance and reet training arrays\n",
    "#df = df.query('(ID != \"\")').sample(n=5)\n",
    "#Remove 10% of negative images to balance dataset\n",
    "train_image_df = train_image_df.drop(train_image_df[train_image_df['label'] == 0].sample(fract = .1).index)\n",
    "\n",
    "#Convert to Numpy Array\n",
    "train_image_id = train_image_df[\"id\"].to_numpy()\n",
    "test_image_id =  test_image_dff[\"id\"].to_numpy()\n",
    "train_image_label = train_image_df[\"label\"].to_numpy()\n",
    "\n",
    "print('Train Image Id array shape: ', train_image_id.shape)\n",
    "print('Test Image Id array shape: ', test_image_id )\n",
    "print('Train Label Id array shape: ', train_image_id.shape)\n",
    "\n",
    "# Charts, new data shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056beab",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "Based on the EDA and the following approach will be deployed.\n",
    "\n",
    "1. Establish train and test sets.  50,000 images of each classification.\n",
    "2. Preprocess the data (scaling, normalization).\n",
    "3. Compare two models of variaing complexity (optimizer - Adam, metric - accuracy).\n",
    "4. Improve best model through hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5881f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pre-Process Data\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "img_height, img_width = 32, 32\n",
    "SAMPLE_SIZE = 50000\n",
    "RANDOM_STATE = 25\n",
    "\n",
    "#Set Training set size and split\n",
    "negative_training_samples = train_df[train_df[\"label\"] == \"0\"].sample(SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "positive_training_samples = train_df[train_df[\"label\"] == \"1\"].sample(SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "full_train_set = pd.concat([negative_training_samples, positive_training_samples], axis=0).reset_index(drop=True)\n",
    "full_train_set.head()\n",
    "full_train_set.info()\n",
    "\n",
    "training_set, test_set = train_test_split(full_train_set, random_state=RANDOM_STATE, test_size=0.3, shuffle=True, stratify=full_train_set[\"label\"])\n",
    "\n",
    "#Generate and scale images\n",
    "data_image_generator = ImageDataGenerator(featurewise_center=False, zoom_range = 0.2, rotation_range = 30, rescale=1./255)\n",
    "\n",
    "generator_train_set = data_image_generator.flow_from_dataframe(\n",
    "                            dataframe=training_set,\n",
    "                            directory='/train/',\n",
    "                            x_col=\"id\",\n",
    "                            y_col=\"label\",                            \n",
    "                            batch_size=BATCH_SIZE,                           \n",
    "                            seed=RANDOM_STATE,\n",
    "                            class_mode=\"binary\",\n",
    "                            target_size=(32,32))  \n",
    "\n",
    "generator_validation_set = data_image_generator.flow_from_dataframe(\n",
    "                            dataframe=test_set,\n",
    "                            directory='/train/',\n",
    "                            x_col=\"id\",\n",
    "                            y_col=\"label\",\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            seed=RANDOM_STATE,\n",
    "                            class_mode=\"binary\",\n",
    "                            target_size=(32,32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models\n",
    "\n",
    "#Basic Model - two convolution layers, flatten layer, dense layer, batch normalization\n",
    "\n",
    "basic_model = Sequential()\n",
    "\n",
    "basic_model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "basic_model.add(Conv2D(32, (3, 3)))\n",
    "basic_model.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "basic_model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "  \n",
    "basic_model.add(Flatten())\n",
    "basic_model.add(Dense(128))\n",
    "mobasic_modeldel.add(BatchNormalization())\n",
    "basic_model.add(Activation('relu'))\n",
    "\n",
    "basic_model.add(Dense(1))\n",
    "basic_model.add(Activation('sigmoid'))\n",
    "\n",
    "basic_model.summary()\n",
    "\n",
    "#basic_optimizer = Adam(learning_rate=0.0001)\n",
    "#basic_model.compile(loss='binary_crossentropy', optimizer=basic_optimizer, metrics=['accuracy'])\n",
    "basic_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "#Complex Model - 4 convolution layers, flatten layer, dense layer, batch normalization, two dropout layers\n",
    "\n",
    "complex_model = Sequential()\n",
    "\n",
    "complex_model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "complex_model.add(BatchNormalization())\n",
    "complex_model.add(Activation('relu'))\n",
    "complex_model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "complex_model.add(Conv2D(32, (3, 3)))\n",
    "complex_model.add(BatchNormalization())\n",
    "complex_model.add(Activation('relu'))\n",
    "complex_model.add(Dropout(0.15))\n",
    "complex_model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "\n",
    "complex_model.add(Conv2D(32, (3, 3)))\n",
    "complex_model.add(BatchNormalization())\n",
    "complex_model.add(Activation('relu'))\n",
    "complex_model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "complex_model.add(Conv2D(32, (3, 3)))\n",
    "complex_model.add(BatchNormalization())\n",
    "complex_model.add(Activation('relu'))\n",
    "complex_model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "  \n",
    "complex_model.add(Flatten())\n",
    "complex_model.add(Dense(128))\n",
    "complex_model.add(BatchNormalization())\n",
    "complex_model.add(Dropout(0.25))\n",
    "complex_model.add(Activation('relu'))\n",
    "\n",
    "complex_model.add(Dense(1))\n",
    "complex_model.add(Activation('sigmoid'))\n",
    "\n",
    "complex_model.summary()\n",
    "\n",
    "#complex_optimizer = Adam(learning_rate=0.0001)\n",
    "#basic_model.compile(loss='binary_crossentropy', optimizer=complex_optimizer, metrics=['accuracy'])\n",
    "basic_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230ae6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Train models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m STEP_SIZE_TRAIN\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m      4\u001b[0m STEP_SIZE_VALID\u001b[38;5;241m=\u001b[39mvalidation_generator\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mvalidation_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m      5\u001b[0m basic_history \u001b[38;5;241m=\u001b[39m basic_model\u001b[38;5;241m.\u001b[39mfit(generator_train_set,\n\u001b[0;32m      6\u001b[0m                     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m , \n\u001b[0;32m      7\u001b[0m                     steps_per_epoch\u001b[38;5;241m=\u001b[39mSTEP_SIZE_TRAIN,\n\u001b[0;32m      8\u001b[0m                     validation_data \u001b[38;5;241m=\u001b[39m generator_validation_set,\n\u001b[0;32m      9\u001b[0m                     validation_steps\u001b[38;5;241m=\u001b[39mSTEP_SIZE_VALID)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "#Train models\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "basic_history = basic_model.fit(generator_train_set,\n",
    "                    epochs = 20 , \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data = generator_validation_set,\n",
    "                    validation_steps=STEP_SIZE_VALID)\n",
    "\n",
    "\n",
    "complex_history = complex_model.fit(generator_train_set,\n",
    "                    epochs = 20 , \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data = generator_validation_set,\n",
    "                    validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc444b2e",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Compare Results\n",
    "\n",
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0fc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd09ce",
   "metadata": {},
   "source": [
    "Instructions: Step 1\n",
    "less \n",
    " Brief description of the problem and data (5 pts) \n",
    "\n",
    "Briefly describe the challenge problem and NLP. Describe the size, dimension, structure, etc., of the data. \n",
    "\n",
    "Instructions: Step 2\n",
    "less \n",
    "Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data (15 pts)\n",
    "\n",
    "Show a few visualizations like histograms. Describe any data cleaning procedures. Based on your EDA, what is your plan of analysis? \n",
    "\n",
    "Instructions: Step 3\n",
    "less \n",
    "DModel Architecture (25 pts)\n",
    "\n",
    "escribe your model architecture and reasoning for why you believe that specific architecture would be suitable for this problem. Compare multiple architectures and tune hyperparameters. \n",
    "\n",
    "Instructions: Step 4\n",
    "less \n",
    "Results and Analysis (35 pts) \n",
    "\n",
    "Run hyperparameter tuning, try different architectures for comparison, apply techniques to improve training or performance, and discuss what helped.\n",
    "\n",
    "Includes results with tables and figures. There is an analysis of why or why not something worked well, troubleshooting, and a hyperparameter optimization procedure summary.\n",
    "\n",
    "Instructions: Step 5\n",
    "less \n",
    "Conclusion (15 pts)\n",
    "\n",
    "Discuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future?\n",
    "\n",
    "Instructions: Step 6\n",
    "less \n",
    "Produce Deliverables: High-Quality, Organized Jupyter Notebook Report, GitHub Repository, and screenshot of Kaggle leaderboard (35 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.3 Clean the data / Data Preprocessing\n",
    "# check the data format\n",
    "print(K.image_data_format()) \n",
    "channels_last\n",
    "# set random state\n",
    "RANDOM_STATE = 42\n",
    "# set batch size\n",
    "BATCH_SIZE = 10\n",
    "# set input shape\n",
    "img_width, img_height = 64, 64\n",
    "input_shape = (img_width, img_height, 3)\n",
    "Let's balance the data and split train dataset to:\n",
    "\n",
    "train set: is the set used for training model\n",
    "validation set: is the set used during the model training to adjust the hyperparameters. (20%)\n",
    "# balance the data\n",
    "SAMPLE = 80000\n",
    "train1 = train_df[train_df[\"label\"] == \"0\"].sample(SAMPLE, random_state=RANDOM_STATE)\n",
    "train2 = train_df[train_df[\"label\"] == \"1\"].sample(SAMPLE, random_state=RANDOM_STATE)\n",
    "train_dt = pd.concat([train1, train2], axis=0).reset_index(drop=True)\n",
    "train_dt[\"label\"].value_counts()\n",
    "0    80000\n",
    "1    80000\n",
    "Name: label, dtype: int64\n",
    "# split train dataset to train and validation set\n",
    "train_data, valid_data = train_test_split(train_dt,                                                      \n",
    "                                   random_state=RANDOM_STATE, \n",
    "                                   test_size=0.2, \n",
    "                                   shuffle=True, stratify=train_dt[\"label\"])\n",
    "\n",
    "# check value count in train and validation set\n",
    "print(train_data[\"label\"].value_counts())\n",
    "print(valid_data[\"label\"].value_counts())\n",
    "0    64000\n",
    "\n",
    "1    64000\n",
    "\n",
    "Name: label, dtype: int64\n",
    "\n",
    "0    16000\n",
    "\n",
    "1    16000\n",
    "\n",
    "Name: label, dtype: int64\n",
    "Before we can proceed with building the model:\n",
    "\n",
    "The first step to working with neural networks is to normalize the dataset, otherwise, it could take a lot longer for the network to converge on a solution.\n",
    "\n",
    "The usual way of normalizing a dataset is to scale the features, and this is done by subtracting the mean from each feature and dividing by the standard deviation. This will put the features on the same scale somewhere between 0 — 1.\n",
    "\n",
    "As we are working with 32 x 32 NumPy arrays representing each image and each pixel in the array has an intensity somewhere between 1 — 255, a simpler way of getting all of these images on a scale between 0–1 is to divide each array by 255.\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset                           \n",
    "                             zoom_range = 0.2, # Randomly zoom image \n",
    "                             rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                             width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                             height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                             horizontal_flip = True,  # randomly flip images\n",
    "                             rescale=1./255)    # multiply the data by the value provided\n",
    "                            \n",
    "    \n",
    "                           \n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "                            dataframe=train_data,\n",
    "                            directory=train_path,\n",
    "                            x_col=\"id\",\n",
    "                            y_col=\"label\",                            \n",
    "                            batch_size=BATCH_SIZE,                           \n",
    "                            seed=RANDOM_STATE,\n",
    "                            class_mode=\"binary\",\n",
    "                            target_size=(64,64))  \n",
    "Found 128000 validated image filenames belonging to 2 classes.\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "                            dataframe=valid_data,\n",
    "                            directory=train_path,\n",
    "                            x_col=\"id\",\n",
    "                            y_col=\"label\",\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            seed=RANDOM_STATE,\n",
    "                            class_mode=\"binary\",\n",
    "                            target_size=(64,64))\n",
    "Found 32000 validated image filenames belonging to 2 classes.\n",
    "Step 3: Describe Model Architecture\n",
    "Model 1:\n",
    "Model is comprised of:\n",
    "\n",
    "A simple CNN model with 3 Convolutional layers followed by max-pooling layers. A dropout layer is added at the final convolutional layer to avoid overfitting. BatchNormalization normalize the activation of the previous layer at each batch. Sigmoid is used as the activation function for the final layer of the binary classifier. Use binary-entropy loss function for our binary-class classification problem. For simplicity, use accuracy as our evaluation metrics to evaluate the model during training and testing.\n",
    "\n",
    "optimization: Adam\n",
    "learning rate: 0.0001\n",
    "hidden layer activations: relu\n",
    "final layer dropout: 0.4\n",
    "final layer activation: sigmoid because of the binary classification\n",
    "    \n",
    "model = Sequential()\n",
    "# first convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# second convolutional layer\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "# third convolutional layer\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Out layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "Model: \"sequential\"\n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    " Layer (type)                Output Shape              Param #   \n",
    "\n",
    "=================================================================\n",
    "\n",
    " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
    "\n",
    "                                                                 \n",
    "\n",
    " batch_normalization (BatchN  (None, 62, 62, 32)       128       \n",
    "\n",
    " ormalization)                                                   \n",
    "\n",
    "                                                                 \n",
    "\n",
    " activation (Activation)     (None, 62, 62, 32)        0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
    "\n",
    " )                                                               \n",
    "\n",
    "                                                                 \n",
    "\n",
    " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
    "\n",
    "                                                                 \n",
    "\n",
    " batch_normalization_1 (Batc  (None, 29, 29, 64)       256       \n",
    "\n",
    " hNormalization)                                                 \n",
    "\n",
    "                                                                 \n",
    "\n",
    " activation_1 (Activation)   (None, 29, 29, 64)        0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
    "\n",
    " 2D)                                                             \n",
    "\n",
    "                                                                 \n",
    "\n",
    " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
    "\n",
    "                                                                 \n",
    "\n",
    " batch_normalization_2 (Batc  (None, 12, 12, 128)      512       \n",
    "\n",
    " hNormalization)                                                 \n",
    "\n",
    "                                                                 \n",
    "\n",
    " activation_2 (Activation)   (None, 12, 12, 128)       0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
    "\n",
    " 2D)                                                             \n",
    "\n",
    "                                                                 \n",
    "\n",
    " flatten (Flatten)           (None, 4608)              0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    " dense (Dense)               (None, 256)               1179904   \n",
    "\n",
    "\n",
    "                                                                 \n",
    "\n",
    " batch_normalization_3 (Batc  (None, 256)              1024      \n",
    "\n",
    " hNormalization)                                                 \n",
    "\n",
    "                                                                 \n",
    "\n",
    " activation_3 (Activation)   (None, 256)               0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    " dropout (Dropout)           (None, 256)               0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    " dense_1 (Dense)             (None, 1)                 257       \n",
    "\n",
    "                                                                 \n",
    "\n",
    " activation_4 (Activation)   (None, 1)                 0         \n",
    "\n",
    "                                                                 \n",
    "\n",
    "=================================================================\n",
    "\n",
    "Total params: 1,275,329\n",
    "\n",
    "Trainable params: 1,274,369\n",
    "\n",
    "Non-trainable params: 960\n",
    "\n",
    "_________________________________________________________________\n",
    "Let's compile the model now using Adam as our optimizer and binary crossentropy as the loss function. We are using a lower learning rate of 0.0001 for a smoother curve.\n",
    "\n",
    "# compile the model\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "# let’s train our model for 20 epochs\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "history = model.fit(train_generator,\n",
    "                    epochs = 20 , \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID)\n",
    "            \n",
    "Epoch 1/20\n",
    "\n",
    "12800/12800 [==============================] - 697s 54ms/step - loss: 0.4683 - accuracy: 0.7861 - val_loss: 0.3623 - val_accuracy: 0.8436\n",
    "\n",
    "Epoch 2/20\n",
    "\n",
    "12800/12800 [==============================] - 701s 55ms/step - loss: 0.4092 - accuracy: 0.8194 - val_loss: 0.6046 - val_accuracy: 0.7189\n",
    "\n",
    "Epoch 3/20\n",
    "\n",
    "12800/12800 [==============================] - 713s 56ms/step - loss: 0.3906 - accuracy: 0.8296 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
    "\n",
    "Epoch 4/20\n",
    "\n",
    "12800/12800 [==============================] - 729s 57ms/step - loss: 0.3741 - accuracy: 0.8383 - val_loss: 0.3132 - val_accuracy: 0.8693\n",
    "\n",
    "Epoch 5/20\n",
    "\n",
    "12800/12800 [==============================] - 725s 57ms/step - loss: 0.3611 - accuracy: 0.8466 - val_loss: 0.4342 - val_accuracy: 0.7918\n",
    "\n",
    "Epoch 6/20\n",
    "\n",
    "12800/12800 [==============================] - 729s 57ms/step - loss: 0.3494 - accuracy: 0.8520 - val_loss: 0.3308 - val_accuracy: 0.8572\n",
    "\n",
    "Epoch 7/20\n",
    "\n",
    "12800/12800 [==============================] - 701s 55ms/step - loss: 0.3422 - accuracy: 0.8552 - val_loss: 0.4511 - val_accuracy: 0.7863\n",
    "\n",
    "Epoch 8/20\n",
    "\n",
    "12800/12800 [==============================] - 4591s 359ms/step - loss: 0.3346 - accuracy: 0.8604 - val_loss: 0.2900 - val_accuracy: 0.8781\n",
    "\n",
    "Epoch 9/20\n",
    "\n",
    "12800/12800 [==============================] - 2214s 173ms/step - loss: 0.3297 - accuracy: 0.8636 - val_loss: 0.3425 - val_accuracy: 0.8518\n",
    "\n",
    "Epoch 10/20\n",
    "\n",
    "12800/12800 [==============================] - 3386s 265ms/step - loss: 0.3237 - accuracy: 0.8666 - val_loss: 0.3431 - val_accuracy: 0.8586\n",
    "\n",
    "Epoch 11/20\n",
    "\n",
    "12800/12800 [==============================] - 708s 55ms/step - loss: 0.3194 - accuracy: 0.8687 - val_loss: 0.5548 - val_accuracy: 0.7660\n",
    "\n",
    "Epoch 12/20\n",
    "\n",
    "12800/12800 [==============================] - 709s 55ms/step - loss: 0.3146 - accuracy: 0.8711 - val_loss: 0.2735 - val_accuracy: 0.8902\n",
    "\n",
    "Epoch 13/20\n",
    "\n",
    "12800/12800 [==============================] - 708s 55ms/step - loss: 0.3124 - accuracy: 0.8727 - val_loss: 0.3172 - val_accuracy: 0.8640\n",
    "\n",
    "Epoch 14/20\n",
    "\n",
    "12800/12800 [==============================] - 703s 55ms/step - loss: 0.3083 - accuracy: 0.8742 - val_loss: 0.2580 - val_accuracy: 0.8932\n",
    "\n",
    "Epoch 15/20\n",
    "\n",
    "12800/12800 [==============================] - 713s 56ms/step - loss: 0.3032 - accuracy: 0.8768 - val_loss: 0.2570 - val_accuracy: 0.8950\n",
    "\n",
    "Epoch 16/20\n",
    "\n",
    "12800/12800 [==============================] - 709s 55ms/step - loss: 0.2990 - accuracy: 0.8789 - val_loss: 0.3435 - val_accuracy: 0.8635\n",
    "\n",
    "Epoch 17/20\n",
    "\n",
    "12800/12800 [==============================] - 701s 55ms/step - loss: 0.2997 - accuracy: 0.8790 - val_loss: 0.2426 - val_accuracy: 0.9014\n",
    "\n",
    "Epoch 18/20\n",
    "\n",
    "12800/12800 [==============================] - 698s 55ms/step - loss: 0.2958 - accuracy: 0.8807 - val_loss: 0.3165 - val_accuracy: 0.8632\n",
    "\n",
    "Epoch 19/20\n",
    "\n",
    "12800/12800 [==============================] - 699s 55ms/step - loss: 0.2929 - accuracy: 0.8824 - val_loss: 0.2443 - val_accuracy: 0.9023\n",
    "\n",
    "Epoch 20/20\n",
    "\n",
    "12800/12800 [==============================] - 703s 55ms/step - loss: 0.2905 - accuracy: 0.8821 - val_loss: 0.2681 - val_accuracy: 0.8869\n",
    "model.save(\"../Week3/my_model1\")\n",
    "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
    "INFO:tensorflow:Assets written to: ../Week3/my_model1/assets\n",
    "INFO:tensorflow:Assets written to: ../Week3/my_model1/assets\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(20)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "Model 2\n",
    "Next, let's use Earlystopping to avoid overfitting by terminating the process early. Since the goal of a training is to minimize the loss. With this, we can set up the metric as:\n",
    "\n",
    "+ monitor : val_loss, value being monitored.  \n",
    "+ mode: min, training will stop when the quantity monitored has stopped decreasing.   \n",
    "+ patience: 3, number of epochs with no improvement after which training will be stopped.  \n",
    "\n",
    "Moreover, let's use Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n",
    "\n",
    "+ factor: factor by which the learning rate will be reduced (new_learning_rate = learning_rate * factor).  \n",
    "+ min_lr: lower bound on the learning rate.  \n",
    "\n",
    "A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.\n",
    "\n",
    "# define an Earlystopping\n",
    "checkpoint_filepath = '../Week3/checkpoint'\n",
    "mp= tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True,\n",
    "                               verbose=1, save_best_only=True)\n",
    "es= tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n",
    "                                   verbose=1, mode='min', min_lr=0.00001)\n",
    "callback=[es, mp, reduce_lr]\n",
    "new_model = Sequential()\n",
    "# first convolutional layer\n",
    "new_model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Activation('relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# second convolutional layer\n",
    "\n",
    "new_model.add(Conv2D(64, (3, 3)))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Activation('relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "# third convolutional layer\n",
    "new_model.add(Conv2D(128, (3, 3)))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Activation('relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(256))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Activation('relu'))\n",
    "new_model.add(Dropout(0.4))\n",
    "\n",
    "# Out layer\n",
    "new_model.add(Dense(1))\n",
    "new_model.add(Activation('sigmoid'))\n",
    "\n",
    "new_model.summary()\n",
    "Model: \"sequential_1\"\n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    " Layer (type)                Output Shape              Param #   \n",
    "\n",
    "=================================================================\n",
    "\n",
    " conv2d_3 (Conv2D)           (None, 62, 62, 32)        896       \n",
    "\n",
    "                                                        \n",
    "        \n",
    "=================================================================\n",
    "\n",
    "Total params: 1,275,329\n",
    "\n",
    "Trainable params: 1,274,369\n",
    "\n",
    "Non-trainable params: 960\n",
    "\n",
    "_________________________________________________________________\n",
    "# compile new model\n",
    "new_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "# let’s train our model for 20 epochs\n",
    "new_history = new_model.fit(train_generator,\n",
    "                    epochs = 20 , \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    callbacks=callback)\n",
    "Epoch 1/20\n",
    "\n",
    "12800/12800 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.7863\n",
    "\n",
    "Epoch 1: val_loss improved from inf to 0.44097, saving model to ../Week3/checkpoint\n",
    "\n",
    "12800/12800 [==============================] - 708s 55ms/step - loss: 0.4709 - accuracy: 0.7863 - val_loss: 0.4410 - val_accuracy: 0.8045 - lr: 1.0000e-04\n",
    "\n",
    "Epoch 2/20\n",
    "\n",
    "12800/12800 [==============================] - ETA: 0s - loss: 0.40\n",
    "        \n",
    "new_model.save(\"../Week3/my_model2\")\n",
    "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
    "INFO:tensorflow:Assets written to: ../Week3/my_model2/assets\n",
    "INFO:tensorflow:Assets written to: ../Week3/my_model2/assets\n",
    "new_acc = new_history.history['accuracy']\n",
    "new_val_acc = new_history.history['val_accuracy']\n",
    "new_loss = new_history.history['loss']\n",
    "new_val_loss = new_history.history['val_loss']\n",
    "epochs_range = range(7)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, new_acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, new_val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, new_loss, label='Training Loss')\n",
    "plt.plot(epochs_range, new_val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Model 1\n",
    "val_loss1, val_acc1 = model.evaluate(validation_generator)\n",
    "print('val_loss_model1:', val_loss1)\n",
    "print('val_acc_model1:', val_acc1)\n",
    "3200/3200 [==============================] - 102s 32ms/step - loss: 0.2670 - accuracy: 0.8864\n",
    "\n",
    "val_loss_model1: 0.26701804995536804\n",
    "\n",
    "val_acc_model1: 0.8864062428474426\n",
    "# predict validation dataset\n",
    "predictions1 = model.predict(validation_generator, verbose=1)\n",
    "predictions1\n",
    "3200/3200 [==============================] - 101s 32ms/step\n",
    "array([[0.03428283],\n",
    "       [0.14390497],\n",
    "       [0.05418937],\n",
    "       ...,\n",
    "       [0.1562683 ],\n",
    "       [0.99227035],\n",
    "       [0.23418935]], dtype=float32)\n",
    "# calculate auc_score\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_true, predictions1, pos_label=1)\n",
    "auc_score1 = auc(fpr1, tpr1)\n",
    "auc_score1\n",
    "0.50013395703125\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='blue')\n",
    "plt.plot(fpr1, tpr1, label='area = {:.2f}'.format(auc_score1))\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "We can print out the classification report to see the precision and accuracy.\n",
    "\n",
    "# Get the prediction binary\n",
    "y_pred1 = np.where(predictions1 > 0.5, 1, 0)\n",
    "# print out the classification report\n",
    "print(classification_report(y_true, y_pred1, target_names = ['no_tumor_tissue (Class 0)','has_tumor_tissue (Class 1)']))\n",
    "                            precision    recall  f1-score   support\n",
    "\n",
    "\n",
    "\n",
    " no_tumor_tissue (Class 0)       0.50      0.56      0.53     16000\n",
    "\n",
    "has_tumor_tissue (Class 1)       0.50      0.45      0.47     16000\n",
    "\n",
    "\n",
    "\n",
    "                  accuracy                           0.50     32000\n",
    "\n",
    "                 macro avg       0.50      0.50      0.50     32000\n",
    "\n",
    "              weighted avg       0.50      0.50      0.50     32000\n",
    "\n",
    "\n",
    "# print out the confusion matrix\n",
    "cm1 = confusion_matrix(y_true, y_pred1)\n",
    "sns.heatmap(cm1, annot=True, fmt=\".0f\")\n",
    "<AxesSubplot:>\n",
    "    \n",
    "<AxesSubplot:>\n",
    "\n",
    "Model 2\n",
    "# the best epoch will be used.\n",
    "new_model.load_weights('../Week3/checkpoint')\n",
    "val_loss2, val_acc2 = new_model.evaluate(validation_generator)\n",
    "print('val_loss_model2:', val_loss2)\n",
    "print('val_acc_model2:', val_acc2)\n",
    "3200/3200 [==============================] - 101s 32ms/step - loss: 0.3265 - accuracy: 0.8601\n",
    "\n",
    "val_loss_model2: 0.32647979259490967\n",
    "\n",
    "val_acc_model2: 0.8601250052452087\n",
    "# predict validation dataset\n",
    "predictions2 = new_model.predict(validation_generator)\n",
    "predictions2\n",
    "3200/3200 [==============================] - 102s 32ms/step\n",
    "array([[0.432638  ],\n",
    "       [0.18982337],\n",
    "       [0.81373936],\n",
    "       ...,\n",
    "       [0.98624164],\n",
    "       [0.12874842],\n",
    "       [0.6048674 ]], dtype=float32)\n",
    "\n",
    "\n",
    "# calculate auc_score\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_true, predictions2, pos_label=1)\n",
    "auc_score2 = auc(fpr2, tpr2)\n",
    "auc_score2\n",
    "0.499510451171875\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='blue')\n",
    "plt.plot(fpr2, tpr2, label='area = {:.2f}'.format(auc_score2))\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "# Get the prediction binary\n",
    "y_pred2 = np.where(predictions2 > 0.5, 1, 0)\n",
    "# print out the classification report\n",
    "print(classification_report(y_true, y_pred2, target_names = ['no_tumor_tissue (Class 0)','has_tumor_tissue (Class 1)']))\n",
    "                            precision    recall  f1-score   support\n",
    "\n",
    "\n",
    "\n",
    " no_tumor_tissue (Class 0)       0.50      0.47      0.49     16000\n",
    "\n",
    "has_tumor_tissue (Class 1)       0.50      0.53      0.52     16000\n",
    "\n",
    "\n",
    "\n",
    "                  accuracy                           0.50     32000\n",
    "    \n",
    "    \n",
    "Predict test data and print out the submission\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                            dataframe=test_df,\n",
    "                            directory=test_path,\n",
    "                            x_col=\"id\",\n",
    "                            y_col=None,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            seed=RANDOM_STATE,\n",
    "                            class_mode=None,\n",
    "                            target_size=(64,64))\n",
    "Found 57458 validated image filenames.\n",
    "# predict validation dataset\n",
    "t_predictions = new_model.predict(test_generator, verbose=1)\n",
    "t_predictions\n",
    "5746/5746 [==============================] - 108s 19ms/step\n",
    "array([[0.13753963],\n",
    "       [0.0328943 ],\n",
    "       [0.17902558],\n",
    "       ...,\n",
    "       [0.03642212],\n",
    "       [0.02054871],\n",
    "       [0.4093984 ]], dtype=float32)\n",
    "# Get the new prediction binary\n",
    "test_pred = np.where(t_predictions > 0.5, 1, 0)\n",
    "# create submission dataframe\n",
    "test_predictions = np.transpose(test_pred)[0]\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id'].apply(lambda x: x.split('.')[0])\n",
    "submission['label'] = test_predictions\n",
    "submission.head()\n",
    "\n",
    "\tid\tlabel\n",
    "0\tfd0a060ef9c30c9a83f6b4bfb568db74b099154d\t0\n",
    "1\t1f9ee06f06d329eb7902a2e03ab3835dd0484581\t0\n",
    "2\t19709bec800f372d0b1d085da6933dd3ef108846\t0\n",
    "3\t7a34fc34523063f13f0617f7518a0330f6187bd3\t0\n",
    "4\t93be720ca2b95fe2126cf2e1ed752bd759e9b0ed\t0\n",
    "# view test prediction counts\n",
    "submission['label'].value_counts()\n",
    "0    43117\n",
    "1    14341\n",
    "Name: label, dtype: int64\n",
    "# plot the count of each label\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.countplot(data=submission, y='label', ax=ax).set(title='\\nFigure 5. The Count of Each Label\\n')\n",
    "\n",
    "# plot the proportion of each label\n",
    "labels = submission['label'].unique().tolist()\n",
    "counts = submission['label'].value_counts()\n",
    "sizes = [counts[v] for v in labels]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%0.2f%%')\n",
    "ax1.axis('equal')\n",
    "plt.title(\"\\nFigure 6. The Proportion of Each Label\\n\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# convert to csv to submit to competition\n",
    "#submission.to_csv('submission.csv', index=False)\n",
    "submission.to_csv('../Week3/submission.csv', index=False)\n",
    "Step 5: Conclusion\n",
    "compare_table = pd.DataFrame({\"Model\": [\"Model1\", \"Model2\"],\n",
    "                        \"val_acc\": [round(val_acc1, 3), round(val_acc2, 3)],\n",
    "                        \"val_loss\": [round(val_loss1, 3), round(val_loss2, 3)],\n",
    "                        \"AUC\": [round(auc_score1, 2), round(auc_score2, 2)]})\n",
    "compare_table\n",
    "Model\tval_acc\tval_loss\tAUC\n",
    "0\tModel1\t0.886\t0.267\t0.5\n",
    "1\tModel2\t0.860\t0.326\t0.5\n",
    "Model 1 has the higher validation accuracy and lower validation loss compare to model2, However AUC score of two \n",
    "models are just the same although it took more time to run model1 than model2 because model2 used Earlystopping and\n",
    "Reduce Learning Rate to optimize the model. I think these two models might be overfitting, so besides these two models,\n",
    "I tried building some models with different learning rate and different values of dense, drop out. For example,\n",
    "when I chose a learning rate like 0.00001, I observed that the model just ran and ended up with an early stop \n",
    "at epoch 4 because of the learning rate was too small, so it was stuck at epoch 4. But due to the limitation \n",
    "of time and memory, I could just build these simple CNN models and get AUC of 0.5. Hence, I believe that \n",
    "there are many ways could improve the result such as run this model by increasing the number of epochs or \n",
    "trying to test with many different parameters might get better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
